{
    "docs": [
        {
            "location": "/", 
            "text": "Doing Projection Mapping\n\n\n  \n\n\nIntroduction\n\n\nThis document is a write-down for a workshop by \nFabian Mor\u00f3n Zirfas\n @ \nthe University of Applied Sciences Potsdam (Germany)\n as part of the seminar \nDatenobjekte\n (data objects) by \nProfessor Boris M\u00fcller\n (a.k.a \n@borism\n on Twitter). We will explore the basics possibilites we have in the field of projection mapping. \nThis is still work in progress, there might be\n \ud83d\udc1b\n,\n \ud83d\udc09 \nand\n \ud83d\udc7e\n.\n  \n\n\nPrerequisites\n\n\n\n\na computer  \n\n\na projector  \n\n\nProcessing  \n\n\nProcessing Keystone library\n\n\nProcessing Syphon library (Mac)  \n\n\nProcessing Spout Setup (Win)  \n\n\nHeavyM  \n\n\n\n\n\n\nNote\n\n\nSome of these are optional. Depending on which workflow you are going to use.  \n\n\n\n\nSource\n\n\nThese docs are written using \nmkdocs\n using the \nreadthedocs\n theme by Fabian Mor\u00f3n Zirfas with \u2665. See the source on \nGitHub.\n  \n\n\nContribution \n Issues\n\n\nPlease file any issues \nhere\n on GitHub. Contributions and Pull Requests are welcome. For quick questions you can also visit our gitter chat.  \n\n\n  \n\n\nCreating gifs\n\n\nThe gifs in these docs are created by transfomring a screenrecord with ffmpeg to a sequence. This sequence then gets transformed with ImageMagick to a gif. ffmpeg and ImageMagick can be installde using \nhomebrew\n.  \n\n\nInstall brew like discribed on their site. Then run:\n\n\nbrew install ffmpeg --with-fdk-aac --with-ffplay --with-freetype --with-frei0r --with-libass --with-libvo-aacenc --with-libvorbis --with-libvpx --with-opencore-amr --with-openjpeg --with-opus --with-rtmpdump --with-schroedinger --with-speex --with-theora --with-tools\nbrew install imagemagick\n\n\n\nYou should be good to go. \n\n\nffmpeg -i blender-ui.mp4 -r 20 -vcodec ppm  -s800x600 seq/out%05d.png\nconvert -layers Optimize seq/out*.png ../images/blender-ui.gif", 
            "title": "Home"
        }, 
        {
            "location": "/#doing-projection-mapping", 
            "text": "", 
            "title": "Doing Projection Mapping"
        }, 
        {
            "location": "/#introduction", 
            "text": "This document is a write-down for a workshop by  Fabian Mor\u00f3n Zirfas  @  the University of Applied Sciences Potsdam (Germany)  as part of the seminar  Datenobjekte  (data objects) by  Professor Boris M\u00fcller  (a.k.a  @borism  on Twitter). We will explore the basics possibilites we have in the field of projection mapping.  This is still work in progress, there might be  \ud83d\udc1b ,  \ud83d\udc09  and  \ud83d\udc7e .", 
            "title": "Introduction"
        }, 
        {
            "location": "/#prerequisites", 
            "text": "a computer    a projector    Processing    Processing Keystone library  Processing Syphon library (Mac)    Processing Spout Setup (Win)    HeavyM      Note  Some of these are optional. Depending on which workflow you are going to use.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/#source", 
            "text": "These docs are written using  mkdocs  using the  readthedocs  theme by Fabian Mor\u00f3n Zirfas with \u2665. See the source on  GitHub.", 
            "title": "Source"
        }, 
        {
            "location": "/#contribution-issues", 
            "text": "Please file any issues  here  on GitHub. Contributions and Pull Requests are welcome. For quick questions you can also visit our gitter chat.", 
            "title": "Contribution &amp; Issues"
        }, 
        {
            "location": "/#creating-gifs", 
            "text": "The gifs in these docs are created by transfomring a screenrecord with ffmpeg to a sequence. This sequence then gets transformed with ImageMagick to a gif. ffmpeg and ImageMagick can be installde using  homebrew .    Install brew like discribed on their site. Then run:  brew install ffmpeg --with-fdk-aac --with-ffplay --with-freetype --with-frei0r --with-libass --with-libvo-aacenc --with-libvorbis --with-libvpx --with-opencore-amr --with-openjpeg --with-opus --with-rtmpdump --with-schroedinger --with-speex --with-theora --with-tools\nbrew install imagemagick  You should be good to go.   ffmpeg -i blender-ui.mp4 -r 20 -vcodec ppm  -s800x600 seq/out%05d.png\nconvert -layers Optimize seq/out*.png ../images/blender-ui.gif", 
            "title": "Creating gifs"
        }, 
        {
            "location": "/workflows/", 
            "text": "There are several different workflows to map video, images or even effects onto objects. Unfortunately the top notch applications are pretty expensive and the open source applications are not that user friendly to use them in a workshop like this. Fortunatly we found an application called \nHeavyM\n which was baked by a Kickstarter campaign. They released a beta version of their application for free and will soon provide an extended version for a reasonable price. For the hackers among us there are also other possibilites like Processing or openFrameworks or a combination of both with Syphon or Spout input output and what not.  \n\n\n\n\nNote\n\n\nKeep in mind that there are limitations we can\nt overcome.\n\n\u261b The depth of an object. A projector has only a limited range where the image is crisp. If you try to distort the projection to much it wont look good.\n\n\u261b The computing power of your machine. To many inputs will spin up your CPU and GPU and everything will be slow.\n\n\u261b The state of the HeavyM software. Even though HeavyM (Live) is good, it is still pretty young. There might be crashes from improper usage or bugs.\n\n\u261b Triangles versus rectangles. 3D objects consist of a lot of triangles. Video or images are rectangular. That means you will have to plan accordingly to create quality distortions. Often it is usefull to distort or mask your image before mapping it onto an real object.  \n\n\n\n\nWorkflow HeavyM Beta\n\n\nThis is the most straight forward way. You can construct from within the interface of HeavyM triangles and rectangles that match your object. There are 20 build in effects and you can have up to 3 different videos that you can load and play on your object. This means you have to plan accordingly and prepare your video footage to match that criteria.  \n\n\n\n\nVideo Editor \u21d2 Video (H264 compression works fine) \n\n\nHeavyM Beta \u21d2 Object  \n\n\n\n\nWorkflow HeavyM Live\n\n\nUsing the Live version is a bit more sophistivated. You also can construct your mapping from within the interface but with a lot more options and enhanced editing capabilities. There are 50 build in effects and you can have a unlimited number of videos. You still need to plan your output but you can have more control over what you are projecting.  \n\n\n\n\nVideo Editor \u21d2 Video (H264 compression works fine) \n\n\nHeavyM Live \u21d2 Object  \n\n\n\n\nWorkflow Processing\n\n\nIf you know how to program you have the possibility to use the \nProcessing\n environment to create your scene. There is robust library called \nKeystone\n that allows you distort and map your output. There are some limitations to that. For example there is no triangular surface. All surfaces are rectangular. This way is not that simple as having a tool like HeavyM but opens up different possibilites. You could for example trigger animations from key strokes or even hack a keyboard to create step or touch triggers. Also using microcontrollers is a possibility. The sky is the limit for this.  \n\n\n\n\nVideo Editor \u21d2 Video (H264 compression works fine)  \n\n\nVideo + Graphics \u21d2 Processing  \n\n\nProcessing \u21d2 Object  \n\n\n\n\nWorkflow HeavyM Live + Syphon/Spout + ?\n\n\n\n\nVideo Editor \u21d2 Video (H264 compression works fine)  \n\n\nVideo + Graphics \u21d2 Syphon/Spout enabled application (e.g. Processing)  \n\n\nSyphon/Spout Server \u21d2 HeavyM  \n\n\nHeavyM \u21d2 Object  \n\n\n\n\nThis approach is also not that straight forward as using just one software tool but gives you endless possibilites of of combinations. \nSyphon\n and \nSpout\n are both technologies that allow applications to share live video or images directly via the GPU. There are a lot of Syphon/Spout enabled applications. For example Resolume, MadMapper, Millumin, After Effects, Quartz Composer and even libraries for Processing and openFrameworks. See their dedicated pages for a full listing of possible applications.  \n\n\n\n\nHint\n\n\nFeel free to contribute any additional information about your workflows on the \nissues of this repo\n or as pull request.", 
            "title": "Workflows"
        }, 
        {
            "location": "/workflows/#workflow-heavym-beta", 
            "text": "This is the most straight forward way. You can construct from within the interface of HeavyM triangles and rectangles that match your object. There are 20 build in effects and you can have up to 3 different videos that you can load and play on your object. This means you have to plan accordingly and prepare your video footage to match that criteria.     Video Editor \u21d2 Video (H264 compression works fine)   HeavyM Beta \u21d2 Object", 
            "title": "Workflow HeavyM Beta"
        }, 
        {
            "location": "/workflows/#workflow-heavym-live", 
            "text": "Using the Live version is a bit more sophistivated. You also can construct your mapping from within the interface but with a lot more options and enhanced editing capabilities. There are 50 build in effects and you can have a unlimited number of videos. You still need to plan your output but you can have more control over what you are projecting.     Video Editor \u21d2 Video (H264 compression works fine)   HeavyM Live \u21d2 Object", 
            "title": "Workflow HeavyM Live"
        }, 
        {
            "location": "/workflows/#workflow-processing", 
            "text": "If you know how to program you have the possibility to use the  Processing  environment to create your scene. There is robust library called  Keystone  that allows you distort and map your output. There are some limitations to that. For example there is no triangular surface. All surfaces are rectangular. This way is not that simple as having a tool like HeavyM but opens up different possibilites. You could for example trigger animations from key strokes or even hack a keyboard to create step or touch triggers. Also using microcontrollers is a possibility. The sky is the limit for this.     Video Editor \u21d2 Video (H264 compression works fine)    Video + Graphics \u21d2 Processing    Processing \u21d2 Object", 
            "title": "Workflow Processing"
        }, 
        {
            "location": "/workflows/#workflow-heavym-live-syphonspout", 
            "text": "Video Editor \u21d2 Video (H264 compression works fine)    Video + Graphics \u21d2 Syphon/Spout enabled application (e.g. Processing)    Syphon/Spout Server \u21d2 HeavyM    HeavyM \u21d2 Object     This approach is also not that straight forward as using just one software tool but gives you endless possibilites of of combinations.  Syphon  and  Spout  are both technologies that allow applications to share live video or images directly via the GPU. There are a lot of Syphon/Spout enabled applications. For example Resolume, MadMapper, Millumin, After Effects, Quartz Composer and even libraries for Processing and openFrameworks. See their dedicated pages for a full listing of possible applications.     Hint  Feel free to contribute any additional information about your workflows on the  issues of this repo  or as pull request.", 
            "title": "Workflow HeavyM Live + Syphon/Spout + ?"
        }, 
        {
            "location": "/heavym-beta/", 
            "text": "The HeavyM Beta is, as described in the \nWorkflow section\n a smaller version of \nHeavyM Live\n. For a lot of use cases this might be enough.\n\nThe application consists of two windows. The editor and the projection. When starting the application you should send the projection into the fullscreen mode on your projector window. Adjust the position of your projector and keep track of where the projector and your object are.\n\nFor the moment we wont look into audio input but there is the possibility to map the sound of your room to different parameters.  \n\n\n\n\n  \n\n\nBasic Usage\n\n\nStart HeavyM Beta, send the projection window into fullscreen and adjust the position of your object.  \n\n\n\n\nHint\n\n\nYou should create some markers for the projector and the object so you can set them up again in the same positions.  \n\n\n\n\nAdding Faces\n\n\nTo add some faces onto your object you need to select from the toolbar on the right the pen and then below the type of face you want to add.\n\n\n\n\nPoint\n\n\nTriangle\n\n\nRectangle\n\n\n\n\nThen just click into the main window. This will add a face of the selected type for each click.  \n\n\n  \n\n\nEditing Faces\n\n\nWhen you have the desired number of faces change into the face editing mode by selecting the bounding box tool below the pen. This will give you the possibility to adjust the corner points of your faces. Make them match your object. For convenience you can activate the magnetic vertex points so your corners fit nicely.  \n\n\n  \n\n\nAdding Effects\n\n\nYou can now add effects to your faces (right hand side, between \nLibrary\n and \nAudio\n). There are several possibilities like adding a stroke or a fill. Making it strobe. Adding an old TV look or for example noise. All these effects get applied to each face in your scene. To have different scenes you can switch on the bottom of the window between scenes. Each effect set is defined per scene. With the play button on the left you can start playing through the scenes. The tempo is defined by the BPM set on the right.  \n\n\n\n\nHint\n\n\nYou can copy paste the whole effect set of one scene to another by right clicking on a scene. There is also the possibility to save a scene into a file and load that file again (on the left side of the scenes row).  \n\n\n\n\n  \n\n\nAdding Media\n\n\nTo add a video or an image you have to add an overlay object. With the scene tab selected you change into the adding mode (the pen on the toolbar) and click once into the main window. Switch to the editing mode (bounding box below the pen) and make the overlay fit your needs. This will hold the media you are adding. Select the media browser on the right and just drag and drop images or videos onto the overlay.  \n\n\n\n\nNote\n\n\n\u261b The video will start playing right away.\n\n\u261b Unfortunately there is no way to start stop videos. This is the same for the Live version. Hopefully this feature will be added soon.\n\n\u261b A H264 compression works fine.", 
            "title": "HeavyM Beta"
        }, 
        {
            "location": "/heavym-beta/#basic-usage", 
            "text": "Start HeavyM Beta, send the projection window into fullscreen and adjust the position of your object.     Hint  You should create some markers for the projector and the object so you can set them up again in the same positions.", 
            "title": "Basic Usage"
        }, 
        {
            "location": "/heavym-beta/#adding-faces", 
            "text": "To add some faces onto your object you need to select from the toolbar on the right the pen and then below the type of face you want to add.   Point  Triangle  Rectangle   Then just click into the main window. This will add a face of the selected type for each click.", 
            "title": "Adding Faces"
        }, 
        {
            "location": "/heavym-beta/#editing-faces", 
            "text": "When you have the desired number of faces change into the face editing mode by selecting the bounding box tool below the pen. This will give you the possibility to adjust the corner points of your faces. Make them match your object. For convenience you can activate the magnetic vertex points so your corners fit nicely.", 
            "title": "Editing Faces"
        }, 
        {
            "location": "/heavym-beta/#adding-effects", 
            "text": "You can now add effects to your faces (right hand side, between  Library  and  Audio ). There are several possibilities like adding a stroke or a fill. Making it strobe. Adding an old TV look or for example noise. All these effects get applied to each face in your scene. To have different scenes you can switch on the bottom of the window between scenes. Each effect set is defined per scene. With the play button on the left you can start playing through the scenes. The tempo is defined by the BPM set on the right.     Hint  You can copy paste the whole effect set of one scene to another by right clicking on a scene. There is also the possibility to save a scene into a file and load that file again (on the left side of the scenes row).", 
            "title": "Adding Effects"
        }, 
        {
            "location": "/heavym-beta/#adding-media", 
            "text": "To add a video or an image you have to add an overlay object. With the scene tab selected you change into the adding mode (the pen on the toolbar) and click once into the main window. Switch to the editing mode (bounding box below the pen) and make the overlay fit your needs. This will hold the media you are adding. Select the media browser on the right and just drag and drop images or videos onto the overlay.     Note  \u261b The video will start playing right away. \n\u261b Unfortunately there is no way to start stop videos. This is the same for the Live version. Hopefully this feature will be added soon. \n\u261b A H264 compression works fine.", 
            "title": "Adding Media"
        }, 
        {
            "location": "/heavym-live/", 
            "text": "HeavyM Live is the extended version of HeavyM Beta. At this point we want to thank the HeavyM Team again for providing us with some trial licenses for our workshop. It provides a lot more functionality than the Beta version. Connect your projector. After you downloaded and installed HeavyM Live you get the window below on startup.  \n\n\n  \n\n\nBasic Usage\n\n\nThe application consists of two windows. The editor window and the projection window. To get the projection window up and running you need to connect and enable your projector by clicking on the projector icon on the top of the window. You will be presented with a screen selection. Select the one that is your secondary screen and hit okay. Your secondary screen should now be a black fullscreen window.  \n\n\n  \n\n\nAdding Faces\n\n\nNow you can now start dragging and droping faces onte the editor window. There are the possibility to add:  \n\n\n\n\nTriangles\n\n\nRectangles\n\n\nEllipses\n\n\nPolygons (free forms)  \n\n\nMedia overlays (we will look into that in the section \nLoading Media\n)\n\n\n\n\nFor easier handling there is button for activating magnetic corners on the faces. it makes your faces snap together and prevents having unwanted holes. Start building your object. You can delete unwanted faces just by selcting them and hitting \u232b (delete). To move your editor window around you can hold space.  \n\n\n  \n\n\n\n\nHint\n\n\nDon\nt forget to save your project regulary to prevent data loss.  \n\n\n\n\n\n\n\n\n\n\nAction\n\n\nDecscription\n\n\n\n\n\n\n\n\n\n\nright click\n\n\noptions for copy pasting, deleting and group assignment\n\n\n\n\n\n\n\u232b\n\n\ndeletes the selected face\n\n\n\n\n\n\nSpace + Mouse movement\n\n\nmoves the editor area\n\n\n\n\n\n\n\n\nAdding Effects\n\n\nWhen you are done adding your faces you can add effects to them. As default only the borders are activated. \n\n\n  \n\n\nOn the right side of the editor window you find options for, borders, fills, shaders and structure. Explore them at will. For our particular use the fill settings are the most important.\n\nYou can activate or deactivate all the effects by clicking the according icon when the additional panel is expanded. It will have an little \u2713 mark when it is activated. See the animation below.  \n\n\n  \n\n\nLoading Media\n\n\nTo add video overlays to your faces you need to add a player face into your scene. Drag and drop (from the top bar) the player object onto your scene. This gives you a rectangular field and opens the media panel on the left side of the window. Hit the little plus sign on the media section and browse for a video.  \n\n\n\n\nNote\n\n\nWe used a H264 compression in a .mov container. If you try other codecs we would be happy to hear from you.  \n\n\n\n\nWhen the media is loaded and you\nve adjusted the bounds of your video hit the play button and your video will start playing in an infinite loop. \nCurrently there is no way to start and stop several videos at once. We hope this will be a feature soon.\n\nSee the animation below.  \n\n\n  \n\n\n\n\nHint\n\n\nYou need to activate the fill property on the right side for all the faces that should have the video as fill. If you just want the whole video on the screen without being cropped by the faces. Hit the \nactivate on top\n button on top of the panel on the left side.  \n\n\n\n\nLoading Syphon Input\n\n\nTo have input from other sources, e.g. After Effects or Processing you just need to follow the instructions in the \nLoading Media section\n. On the media panel on the bottom is a area for Syphon input. Select in the little dropdown menu the Syphon server you have already running and hit the play button. Now you should have the frames of your external application on your object. For further information on using Syphon go to the section \nProcessing Syphon\n or \nQuartz Composer Syphon\n.  \n\n\n\n\nWarning\n\n\nStarting the input \nwithout\n having a Syphon server running might crash HeavyM.  \n\n\n\n\nAdditional Notes\n\n\nThere are several more things you can do. We only scratched the surface of HeavyM. You can for example:  \n\n\n\n\nAdd several scenes to have different configurations for your effects and overlays.  \n\n\nRun the scenes per beat.  \n\n\nAdd a grid to transform all the faces within that grid at once.  \n\n\nAssigning faces to groups (allows to have different effect configurations for face groups)\n\n\nLocking faces and media overlays(for better handling).  \n\n\nAnimate borders, lines within the faces shaders and effects.\n\n\nDrive effects from audio or midi input.", 
            "title": "HeavyM Live"
        }, 
        {
            "location": "/heavym-live/#basic-usage", 
            "text": "The application consists of two windows. The editor window and the projection window. To get the projection window up and running you need to connect and enable your projector by clicking on the projector icon on the top of the window. You will be presented with a screen selection. Select the one that is your secondary screen and hit okay. Your secondary screen should now be a black fullscreen window.", 
            "title": "Basic Usage"
        }, 
        {
            "location": "/heavym-live/#adding-faces", 
            "text": "Now you can now start dragging and droping faces onte the editor window. There are the possibility to add:     Triangles  Rectangles  Ellipses  Polygons (free forms)    Media overlays (we will look into that in the section  Loading Media )   For easier handling there is button for activating magnetic corners on the faces. it makes your faces snap together and prevents having unwanted holes. Start building your object. You can delete unwanted faces just by selcting them and hitting \u232b (delete). To move your editor window around you can hold space.         Hint  Don t forget to save your project regulary to prevent data loss.        Action  Decscription      right click  options for copy pasting, deleting and group assignment    \u232b  deletes the selected face    Space + Mouse movement  moves the editor area", 
            "title": "Adding Faces"
        }, 
        {
            "location": "/heavym-live/#adding-effects", 
            "text": "When you are done adding your faces you can add effects to them. As default only the borders are activated.       On the right side of the editor window you find options for, borders, fills, shaders and structure. Explore them at will. For our particular use the fill settings are the most important. \nYou can activate or deactivate all the effects by clicking the according icon when the additional panel is expanded. It will have an little \u2713 mark when it is activated. See the animation below.", 
            "title": "Adding Effects"
        }, 
        {
            "location": "/heavym-live/#loading-media", 
            "text": "To add video overlays to your faces you need to add a player face into your scene. Drag and drop (from the top bar) the player object onto your scene. This gives you a rectangular field and opens the media panel on the left side of the window. Hit the little plus sign on the media section and browse for a video.     Note  We used a H264 compression in a .mov container. If you try other codecs we would be happy to hear from you.     When the media is loaded and you ve adjusted the bounds of your video hit the play button and your video will start playing in an infinite loop.  Currently there is no way to start and stop several videos at once. We hope this will be a feature soon. \nSee the animation below.         Hint  You need to activate the fill property on the right side for all the faces that should have the video as fill. If you just want the whole video on the screen without being cropped by the faces. Hit the  activate on top  button on top of the panel on the left side.", 
            "title": "Loading Media"
        }, 
        {
            "location": "/heavym-live/#loading-syphon-input", 
            "text": "To have input from other sources, e.g. After Effects or Processing you just need to follow the instructions in the  Loading Media section . On the media panel on the bottom is a area for Syphon input. Select in the little dropdown menu the Syphon server you have already running and hit the play button. Now you should have the frames of your external application on your object. For further information on using Syphon go to the section  Processing Syphon  or  Quartz Composer Syphon .     Warning  Starting the input  without  having a Syphon server running might crash HeavyM.", 
            "title": "Loading Syphon Input"
        }, 
        {
            "location": "/heavym-live/#additional-notes", 
            "text": "There are several more things you can do. We only scratched the surface of HeavyM. You can for example:     Add several scenes to have different configurations for your effects and overlays.    Run the scenes per beat.    Add a grid to transform all the faces within that grid at once.    Assigning faces to groups (allows to have different effect configurations for face groups)  Locking faces and media overlays(for better handling).    Animate borders, lines within the faces shaders and effects.  Drive effects from audio or midi input.", 
            "title": "Additional Notes"
        }, 
        {
            "location": "/processing-keystone/", 
            "text": "The \nKeystone\n library, written by \nDavid Bouchard\n, is a video projection mapping library. Currently, it allows you to warp your Processing sketches onto any flat surface by using corner pin keystoning, regardless of your projector\ns position and orientation.  \n\n\nProcessing Setup\n\n\nDownload and install Processing 3 and go to \nSketch \n Import Library\u2026 \n Add Library\n. Search in the Library Manager for \nkeystone\n. Hit install and restart Processing.  \n\n\n  \n\n\nNow you should find under \nFile \n Examples \n Contributed Libraries\n a folder called Keystone with an example called \nCornerPin\n. \n\n\nBasic Usage\n\n\nWhen running the CornerPin example you will get presented with a simple screen that contains rectangle with a green ellipse on it that follows the mouse (see the image). \n\n\n  \n\n\nFirst we need to adjust the size of the sketch to our projectors output. Look for following line and put in the width and height of your projector (in that order).\n\n\nsize(800, 600, P3D);\n\n\n\nWhen running our sketch we now will get a window that fits our output. Run it as a fullscreen window by hitting the enlarge button on the top of it or start the sketch in presentation mode dirctly from Processing.  \n\n\n\n\n\n\n\n\nKeycombo\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\u2318 + R\n\n\nruns the sketch\n\n\n\n\n\n\n\u21e7 + \u2318 + R\n\n\nruns the sketch in presentation mode\n\n\n\n\n\n\n\n\nThe next parameter we need to adjust is the line that creates our corner pin surface. Look for the following line and change the values to a desired size or leave them as is. Depending on what your output should be.  \n\n\nsurface = ks.createCornerPinSurface(400, 300, 20);\n\n\n\nThe firs parameter is the \nwidth\n, the second the \nheight\n and the third is the \nresolution\n of the surface. These values need to correspond with the next parameter we need to adjust. \n\n\noffscreen = createGraphics(400, 300, P3D);\n\n\n\nThis is the size of the graphics we are going to generate within Processing. We don\nt render anything directly into the window of the application. Instead the library takes a offscreen rendered graphic and distorts this in the cornerpin surface. That means every graphical output of Processing that we want to have distorted needs to be called as a child of the \noffscreen\n \nPGraphics\n object. See the \nProcessing reference\n for further insight.\n\nThe following lines in the sketch get the transformed mouse position and add the graphical output to the offscreen buffer.  \n\n\nPVector surfaceMouse = surface.getTransformedMouse();\noffscreen.beginDraw();\noffscreen.background(255);\noffscreen.fill(0, 255, 0);\noffscreen.ellipse(surfaceMouse.x, surfaceMouse.y, 75, 75);\noffscreen.endDraw();\n\n\n\nEverything you want on your object needs to be between the lines \noffscreen.beginDraw();\n and \noffscreen.endDraw();\n. After that we send the \nPGraphics\n object to the Keystone surface and render it to the screen with the following line.  \n\n\nsurface.render(offscreen);\n\n\n\nNow you can distort your surface. Hit \nc\n and you will get some handles at each corner. for convenience there are two other commands implemented. \ns\n saves your scene to a .xml file. \nl\n loads a scene from that file.\n\n\n\n\n\n\n\n\nKey\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ns\n\n\nsave scene\n\n\n\n\n\n\nl\n\n\nload scene\n\n\n\n\n\n\nc\n\n\nedit cornerpins\n\n\n\n\n\n\n\n\n  \n\n\nThats mostly it. If you want to map several surfaces you just need to:  \n\n\n\n\ncreate a PGraphics object for each surface\n\n\ncreate a CornerPinSurface object for each surface\n\n\ninitialize them \n\n\ndraw to your offscreen buffers\n\n\nadd them all to the \nsurface.render(PGraphics)\n command.  \n\n\n\n\nExamples\n\n\nIn the repository you can find three examples for processing. To run them you need to \ndownload\n the whole folder and open the .pde files in Processing. See the examples/Processing/Keystone directory for the sketches.  \n\n\nEx. CornerPin\n\n\nThis is the basic example taken from the Keystone library. It distorts one surface as explained above.  \n\n\n\n\n\n\n\n\nKey\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ns\n\n\nsave scene\n\n\n\n\n\n\nl\n\n\nload scene\n\n\n\n\n\n\nc\n\n\nedit cornerpins\n\n\n\n\n\n\n\n\nRead the source code \nhere\n.  \n\n\nEx. CornerPinVideo\n\n\nThis example shows how to use a video inside of your surface. You can play/pause it by hitting \np\n.  \n\n\n  \n\n\n\n\n\n\n\n\nKey\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ns\n\n\nsave scene\n\n\n\n\n\n\nl\n\n\nload scene\n\n\n\n\n\n\nc\n\n\nedit cornerpins\n\n\n\n\n\n\np\n\n\nplay/pause the video\n\n\n\n\n\n\n\n\nRead the source code \nhere\n.  \n\n\nEx. CornerPinMultiSurface\n\n\nThe MultiSurface example shows the usage of two surfaces that build a corner. All the key strokes from the CornerPin examples apply.  \n\n\n  \n\n\n\n\n\n\n\n\nKey\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ns\n\n\nsave scene\n\n\n\n\n\n\nl\n\n\nload scene\n\n\n\n\n\n\nc\n\n\nedit cornerpins\n\n\n\n\n\n\n\n\nRead the source code \nhere\n.  \n\n\nKeystone Reference\n\n\nFor an deeper insight into the Keystone library. Go to \nkeystonep5.sourceforge.net/reference/index.html\n.", 
            "title": "Processing and Keystone"
        }, 
        {
            "location": "/processing-keystone/#processing-setup", 
            "text": "Download and install Processing 3 and go to  Sketch   Import Library\u2026   Add Library . Search in the Library Manager for  keystone . Hit install and restart Processing.        Now you should find under  File   Examples   Contributed Libraries  a folder called Keystone with an example called  CornerPin .", 
            "title": "Processing Setup"
        }, 
        {
            "location": "/processing-keystone/#basic-usage", 
            "text": "When running the CornerPin example you will get presented with a simple screen that contains rectangle with a green ellipse on it that follows the mouse (see the image).       First we need to adjust the size of the sketch to our projectors output. Look for following line and put in the width and height of your projector (in that order).  size(800, 600, P3D);  When running our sketch we now will get a window that fits our output. Run it as a fullscreen window by hitting the enlarge button on the top of it or start the sketch in presentation mode dirctly from Processing.       Keycombo  Description      \u2318 + R  runs the sketch    \u21e7 + \u2318 + R  runs the sketch in presentation mode     The next parameter we need to adjust is the line that creates our corner pin surface. Look for the following line and change the values to a desired size or leave them as is. Depending on what your output should be.    surface = ks.createCornerPinSurface(400, 300, 20);  The firs parameter is the  width , the second the  height  and the third is the  resolution  of the surface. These values need to correspond with the next parameter we need to adjust.   offscreen = createGraphics(400, 300, P3D);  This is the size of the graphics we are going to generate within Processing. We don t render anything directly into the window of the application. Instead the library takes a offscreen rendered graphic and distorts this in the cornerpin surface. That means every graphical output of Processing that we want to have distorted needs to be called as a child of the  offscreen   PGraphics  object. See the  Processing reference  for further insight. \nThe following lines in the sketch get the transformed mouse position and add the graphical output to the offscreen buffer.    PVector surfaceMouse = surface.getTransformedMouse();\noffscreen.beginDraw();\noffscreen.background(255);\noffscreen.fill(0, 255, 0);\noffscreen.ellipse(surfaceMouse.x, surfaceMouse.y, 75, 75);\noffscreen.endDraw();  Everything you want on your object needs to be between the lines  offscreen.beginDraw();  and  offscreen.endDraw(); . After that we send the  PGraphics  object to the Keystone surface and render it to the screen with the following line.    surface.render(offscreen);  Now you can distort your surface. Hit  c  and you will get some handles at each corner. for convenience there are two other commands implemented.  s  saves your scene to a .xml file.  l  loads a scene from that file.     Key  Description      s  save scene    l  load scene    c  edit cornerpins         Thats mostly it. If you want to map several surfaces you just need to:     create a PGraphics object for each surface  create a CornerPinSurface object for each surface  initialize them   draw to your offscreen buffers  add them all to the  surface.render(PGraphics)  command.", 
            "title": "Basic Usage"
        }, 
        {
            "location": "/processing-keystone/#examples", 
            "text": "In the repository you can find three examples for processing. To run them you need to  download  the whole folder and open the .pde files in Processing. See the examples/Processing/Keystone directory for the sketches.", 
            "title": "Examples"
        }, 
        {
            "location": "/processing-keystone/#ex-cornerpin", 
            "text": "This is the basic example taken from the Keystone library. It distorts one surface as explained above.       Key  Description      s  save scene    l  load scene    c  edit cornerpins     Read the source code  here .", 
            "title": "Ex. CornerPin"
        }, 
        {
            "location": "/processing-keystone/#ex-cornerpinvideo", 
            "text": "This example shows how to use a video inside of your surface. You can play/pause it by hitting  p .           Key  Description      s  save scene    l  load scene    c  edit cornerpins    p  play/pause the video     Read the source code  here .", 
            "title": "Ex. CornerPinVideo"
        }, 
        {
            "location": "/processing-keystone/#ex-cornerpinmultisurface", 
            "text": "The MultiSurface example shows the usage of two surfaces that build a corner. All the key strokes from the CornerPin examples apply.           Key  Description      s  save scene    l  load scene    c  edit cornerpins     Read the source code  here .", 
            "title": "Ex. CornerPinMultiSurface"
        }, 
        {
            "location": "/processing-keystone/#keystone-reference", 
            "text": "For an deeper insight into the Keystone library. Go to  keystonep5.sourceforge.net/reference/index.html .", 
            "title": "Keystone Reference"
        }, 
        {
            "location": "/processing-syphon/", 
            "text": "Syphon is a framework, written by \nTom Butterworth\n and \nAnton Marini\n, that allows applications to share frames (full frame rate video or stills) with one another in realtime. In our example we will use Processing to send simple animations to HeavyM. You always will have a \nSyphon Server\n and a \nSyphon Client\n. In our case the server is our Processing applet and the client is HeavyM.  \n\n\nProcessing Setup\n\n\nDownload and install Processing 3 and go to \nSketch \n Import Library\u2026 \n Add Library\n. Search in the Library Manager for \nSyphon\n. Hit install and restart Processing.  \n\n\n  \n\n\nSendScreen\n\n\nThis example is the same as provided by the library\ns author. It sends the whole screen to the client. This example is fairly simple. Just read the \nsource code\n.  \n\n\n  \n\n\n\n\nNote\n\n\nThere are currently some OpenGL problems with this sketch und Processing 3. When trying to send video images and textured 3D PShapes to the client it does not generate any output on the client side. If you want video or textured PShapes take a look at the SyphonMoviePlayer example.  \n\n\n\n\nSyphonMoviePlayer\n\n\nTo be able to control videos from key inputs we can use Processing to play the video. Then we send the single frames to the client(s). In this case we are going to draw the video frame by frame into an offscreen buffer (a PGraphics object) and then send these buffered images to the client(s). The process is pretty similar to what is done with the \nKeystone library\n.  \n\n\n\n\n\n\n\n\nKey\n\n\nAction\n\n\n\n\n\n\n\n\n\n\np or P\n\n\nPlay/Pause Video\n\n\n\n\n\n\n\n\n  \n\n\nRead the \nsource code here\n.  \n\n\nSyphonTexturedShape\n\n\nThis sketch shows how to load an image as a texture for a distorted PShape. The frames also get written into an offscreen buffer. Then they are passed on to the client(s). This allows us to distort an image or shape in Processing before sending it to HeavyM.  \n\n\n  \n\n\n  \n\n\nRead the \nsource code here\n.", 
            "title": "Processing and Syphon"
        }, 
        {
            "location": "/processing-syphon/#processing-setup", 
            "text": "Download and install Processing 3 and go to  Sketch   Import Library\u2026   Add Library . Search in the Library Manager for  Syphon . Hit install and restart Processing.", 
            "title": "Processing Setup"
        }, 
        {
            "location": "/processing-syphon/#sendscreen", 
            "text": "This example is the same as provided by the library s author. It sends the whole screen to the client. This example is fairly simple. Just read the  source code .         Note  There are currently some OpenGL problems with this sketch und Processing 3. When trying to send video images and textured 3D PShapes to the client it does not generate any output on the client side. If you want video or textured PShapes take a look at the SyphonMoviePlayer example.", 
            "title": "SendScreen"
        }, 
        {
            "location": "/processing-syphon/#syphonmovieplayer", 
            "text": "To be able to control videos from key inputs we can use Processing to play the video. Then we send the single frames to the client(s). In this case we are going to draw the video frame by frame into an offscreen buffer (a PGraphics object) and then send these buffered images to the client(s). The process is pretty similar to what is done with the  Keystone library .       Key  Action      p or P  Play/Pause Video         Read the  source code here .", 
            "title": "SyphonMoviePlayer"
        }, 
        {
            "location": "/processing-syphon/#syphontexturedshape", 
            "text": "This sketch shows how to load an image as a texture for a distorted PShape. The frames also get written into an offscreen buffer. Then they are passed on to the client(s). This allows us to distort an image or shape in Processing before sending it to HeavyM.            Read the  source code here .", 
            "title": "SyphonTexturedShape"
        }, 
        {
            "location": "/qc-syphon/", 
            "text": "Quartz Composer Setup\n\n\n\n\nQuartz Composer is a node-based visual programming language provided as part of the Xcode development environment in Mac OS X for processing and rendering graphical data.\nQuartz Composer uses OpenGL (including GLSL), OpenCL (only in Mac OS X 10.6 and later), Core Image, Core Video, JavaScript, and other technologies to build an API and a developer tool around a simple visual programming paradigm. Apple has embedded Quartz technologies deeply into the operating system. Compositions created in Quartz Composer can be played standalone in any QuickTime-aware application (although only on Mac OS X 10.4 and later), as a system Screen Saver, as an iTunes Visualizer, from inside the Quartz Composer application, or can be embedded into a Cocoa or Carbon application via supplied user interface widgets. Because Quartz Composer makes extensive use of hardware acceleration and pixel shaders, it is recommended to have a graphics card with at least 32 MB of memory. While Quartz Composer is included with the iPhone SDK, as of December 2015 there is no way of running Quartz Compositions on iOS devices.  \n\n\n\n\nfrom wikipedia.org\n\n\nWe wont go into depth with this setup (for now). It just included to give you an impression of what is possible with other applications. Use it as a starting point when you are into. QC. The process is nearly the same as with processing. Generate your output and pass the frames to the Syphon Server patch. Of course you need to install the \nQC modules\n first. The .qtz file shown below is provided within the \nexamples\n.", 
            "title": "Quartz Composer and Syphon"
        }, 
        {
            "location": "/qc-syphon/#quartz-composer-setup", 
            "text": "Quartz Composer is a node-based visual programming language provided as part of the Xcode development environment in Mac OS X for processing and rendering graphical data.\nQuartz Composer uses OpenGL (including GLSL), OpenCL (only in Mac OS X 10.6 and later), Core Image, Core Video, JavaScript, and other technologies to build an API and a developer tool around a simple visual programming paradigm. Apple has embedded Quartz technologies deeply into the operating system. Compositions created in Quartz Composer can be played standalone in any QuickTime-aware application (although only on Mac OS X 10.4 and later), as a system Screen Saver, as an iTunes Visualizer, from inside the Quartz Composer application, or can be embedded into a Cocoa or Carbon application via supplied user interface widgets. Because Quartz Composer makes extensive use of hardware acceleration and pixel shaders, it is recommended to have a graphics card with at least 32 MB of memory. While Quartz Composer is included with the iPhone SDK, as of December 2015 there is no way of running Quartz Compositions on iOS devices.     from wikipedia.org  We wont go into depth with this setup (for now). It just included to give you an impression of what is possible with other applications. Use it as a starting point when you are into. QC. The process is nearly the same as with processing. Generate your output and pass the frames to the Syphon Server patch. Of course you need to install the  QC modules  first. The .qtz file shown below is provided within the  examples .", 
            "title": "Quartz Composer Setup"
        }, 
        {
            "location": "/screener-syphon/", 
            "text": "There might the the wish to use applications that are not Syphon enabled. For this we can use \nScreener\n. Screener is a OSX application for capturing whole screens.  \n\n\nTHe application needs to be compiled with XCode on your machine.  \n\n\n  \n\n\nMore to come\u2026", 
            "title": "Screener and Syphon"
        }, 
        {
            "location": "/links/", 
            "text": "HeavyM\n  \n\n\n\n\nProcessing.org\n\n\nProjection Mapping Central - Your one-stop shop for projection mapping and video mapping\n  \n\n\n\n\n\n\nSyphon\n  \n\n\n\n\nSpout\n  \n\n\n\n\nProcessing libraries\n\n\n\n\nSyphon/Processing Library \u00b7 GitHub\n  \n\n\nkeystone/Processing Library \u00b7 Sourceforge\n  \n\n\n\n\nMisc\n\n\n\n\nHow To Project On 3D Geometry vvvv", 
            "title": "Links"
        }, 
        {
            "location": "/links/#processing-libraries", 
            "text": "Syphon/Processing Library \u00b7 GitHub     keystone/Processing Library \u00b7 Sourceforge", 
            "title": "Processing libraries"
        }, 
        {
            "location": "/links/#misc", 
            "text": "How To Project On 3D Geometry vvvv", 
            "title": "Misc"
        }
    ]
}